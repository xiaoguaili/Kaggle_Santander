{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import xgboost as xgb \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import os  # for Macbook\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Loading data...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Started Loading data...')\n",
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the columns and extract the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(370,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.columns.shape)\n",
    "target = train['TARGET']\n",
    "train.drop(['TARGET'], axis = 1, inplace = True)\n",
    "train.columns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicated Features, including zero columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Duplicated Columns....\n",
      "(590,)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Removing Duplicated Columns....')\n",
    "remove = []\n",
    "c = train.columns\n",
    "for i in range(len(c)-1):\n",
    "    v = train[c[i]].values\n",
    "    for j in range(i+1, len(c)):\n",
    "        if np.array_equal(v, train[c[j]].values):\n",
    "            remove.append(c[j])\n",
    "print(np.shape(remove))\n",
    "train.drop(remove, axis=1, inplace=True)\n",
    "test.drop(remove, axis=1, inplace=True)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing constant columns....\n",
      "(0,)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Removing constant columns....')\n",
    "remove = []\n",
    "for col in train.columns:\n",
    "    if len(train[col].unique()) == 0:\n",
    "        remove.append(col)\n",
    "print(np.shape(remove))\n",
    "train.drop(remove, axis=1, inplace=True)\n",
    "test.drop(remove, axis=1, inplace=True)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a feature that counts number of assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 308)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(76020, 309)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_features = train.columns[1:] #为什么排除了最后一个？\n",
    "print(train.shape)\n",
    "train['SumZeros'] = (train[original_features] == 0).sum(axis=1)\n",
    "test['SumZeros'] = (test[original_features] == 0).sum(axis=1)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'var3', 'var15', 'imp_ent_var16_ult1', 'imp_op_var39_comer_ult1',\n",
       "       'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1',\n",
       "       'imp_op_var40_comer_ult3', 'imp_op_var40_efect_ult1',\n",
       "       'imp_op_var40_efect_ult3',\n",
       "       ...\n",
       "       'saldo_medio_var33_hace2', 'saldo_medio_var33_hace3',\n",
       "       'saldo_medio_var33_ult1', 'saldo_medio_var33_ult3',\n",
       "       'saldo_medio_var44_hace2', 'saldo_medio_var44_hace3',\n",
       "       'saldo_medio_var44_ult1', 'saldo_medio_var44_ult3', 'var38',\n",
       "       'SumZeros'],\n",
       "      dtype='object', length=309)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating PCA features\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([train, test], axis = 0)\n",
    "df.shape\n",
    "n_train = train.shape[0] # for split the df back into train, test later\n",
    "n_test = test.shape[0]\n",
    "\n",
    "def generate_PCA_feature(train, test, original_feature, n_components = 4):\n",
    "    \"\"\"\n",
    "    we fit a PCA decomposition model to training set of the shape\n",
    "    : n_train_samples * n_features\n",
    "    and transform on both training set and testing test. \n",
    "    \"\"\"\n",
    "    pca = PCA(n_components = n_components)\n",
    "    # we need to normalize the data before fitting\n",
    "    train_projected = pca.fit_transform(normalize(train[original_features], axis=0))\n",
    "    test_projected = pca.transform(normalize(test[original_features], axis=0))\n",
    "    for i in range(1, n_components + 1):\n",
    "        name = 'PCA{:02d}'.format(i)\n",
    "        train[name] = train_projected[:, i - 1]\n",
    "        test[name] = test_projected[:, i - 1]\n",
    "    return train, test\n",
    "print('Generating PCA features')\n",
    "train, test = generate_PCA_feature(train, test, original_features, n_components = 4)\n",
    "print('Done!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'var3', 'var15', 'imp_ent_var16_ult1', 'imp_op_var39_comer_ult1',\n",
       "       'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1',\n",
       "       'imp_op_var40_comer_ult3', 'imp_op_var40_efect_ult1',\n",
       "       'imp_op_var40_efect_ult3',\n",
       "       ...\n",
       "       'saldo_medio_var44_hace2', 'saldo_medio_var44_hace3',\n",
       "       'saldo_medio_var44_ult1', 'saldo_medio_var44_ult3', 'var38', 'SumZeros',\n",
       "       'PCA01', 'PCA02', 'PCA03', 'PCA04'],\n",
       "      dtype='object', length=313)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns #  more features added, as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated SVD features for nonlinear clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SVD features\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# we perform cross validation to set the n_components\n",
    "def generate_SVD_feature(train, test, original_features, n_components = 5):\n",
    "    \"\"\"\n",
    "    we fit a PCA decomposition model to training set of the shape\n",
    "    : n_train_samples * n_features\n",
    "    and transform on both training set and testing test. \n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components = n_components)\n",
    "    # we need to normalize the data before fitting\n",
    "    train_projected = svd.fit_transform(train[original_features])\n",
    "    test_projected = svd.transform(test[original_features])\n",
    "    for i in range(1, n_components + 1):\n",
    "        name = 'SVD{:02d}'.format(i)\n",
    "        train[name] = train_projected[:, i - 1]\n",
    "        test[name] = test_projected[:, i - 1]\n",
    "    return train, test\n",
    "print('Generating SVD features')\n",
    "train, test = generate_SVD_feature(train, test, original_features, n_components = 5)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 316)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape # 5 more features added, as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold cross validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = 10\n",
    "skf = StratifiedKFold(n_splits = split)\n",
    "skf.get_n_splits(train, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_round = 10\n",
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eta\"] = 0.03 # 如同学习率\n",
    "params[\"subsample\"] = 1  # 随机采样训练样本 训练实例的子采样比\n",
    "params[\"colsample_bytree\"] = 0.7 # 生成树时进行的列采样\n",
    "params[\"silent\"] = 0 #设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息。\n",
    "params[\"max_depth\"] = 5 # 构建树的深度，越大越容易过拟合\n",
    "params[\"min_child_weight\"] = 1\n",
    "# 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "params[\"eval_metric\"] = \"auc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = target # add back the label\n",
    "features = train.columns[1:-1] # exclude ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['var3', 'var15', 'imp_ent_var16_ult1', 'imp_op_var39_comer_ult1',\n",
       "       'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1',\n",
       "       'imp_op_var40_comer_ult3', 'imp_op_var40_efect_ult1',\n",
       "       'imp_op_var40_efect_ult3', 'imp_op_var40_ult1',\n",
       "       ...\n",
       "       'saldo_medio_var44_hace2', 'saldo_medio_var44_hace3',\n",
       "       'saldo_medio_var44_ult1', 'saldo_medio_var44_ult3', 'var38', 'SumZeros',\n",
       "       'PCA01', 'PCA02', 'PCA03', 'PCA04'],\n",
       "      dtype='object', length=312)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'var3', 'var15', 'imp_ent_var16_ult1', 'imp_op_var39_comer_ult1',\n",
       "       'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1',\n",
       "       'imp_op_var40_comer_ult3', 'imp_op_var40_efect_ult1',\n",
       "       'imp_op_var40_efect_ult3',\n",
       "       ...\n",
       "       'saldo_medio_var44_hace3', 'saldo_medio_var44_ult1',\n",
       "       'saldo_medio_var44_ult3', 'var38', 'SumZeros', 'PCA01', 'PCA02',\n",
       "       'PCA03', 'PCA04', 'target'],\n",
       "      dtype='object', length=314)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1\n",
      "[0]\teval-auc:0.806581\ttrain-auc:0.818774\n",
      "[1]\teval-auc:0.812097\ttrain-auc:0.823918\n",
      "[2]\teval-auc:0.811927\ttrain-auc:0.825647\n",
      "[3]\teval-auc:0.818346\ttrain-auc:0.831688\n",
      "[4]\teval-auc:0.820401\ttrain-auc:0.834072\n",
      "[5]\teval-auc:0.818017\ttrain-auc:0.835681\n",
      "[6]\teval-auc:0.816894\ttrain-auc:0.835054\n",
      "[7]\teval-auc:0.819524\ttrain-auc:0.83583\n",
      "[8]\teval-auc:0.822144\ttrain-auc:0.837802\n",
      "[9]\teval-auc:0.822591\ttrain-auc:0.838328\n",
      "Blind Log Loss: 0.4937098970518886\n",
      "Blind ROC: 0.8225910436407082\n",
      "finished a training model\n",
      "fitting on full data set now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoguai/anaconda3/envs/sklearn/lib/python3.6/site-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase memory consumption\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2\n",
      "[0]\teval-auc:0.803856\ttrain-auc:0.820801\n",
      "[1]\teval-auc:0.806752\ttrain-auc:0.824244\n",
      "[2]\teval-auc:0.805828\ttrain-auc:0.826292\n",
      "[3]\teval-auc:0.809933\ttrain-auc:0.830363\n",
      "[4]\teval-auc:0.810826\ttrain-auc:0.833659\n",
      "[5]\teval-auc:0.81062\ttrain-auc:0.834969\n",
      "[6]\teval-auc:0.81135\ttrain-auc:0.835214\n",
      "[7]\teval-auc:0.810702\ttrain-auc:0.834916\n",
      "[8]\teval-auc:0.810871\ttrain-auc:0.835285\n",
      "[9]\teval-auc:0.81134\ttrain-auc:0.837399\n",
      "Blind Log Loss: 0.49409158294034383\n",
      "Blind ROC: 0.8113396320673079\n",
      "finished a training model\n",
      "fitting on full data set now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoguai/anaconda3/envs/sklearn/lib/python3.6/site-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase memory consumption\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3\n",
      "[0]\teval-auc:0.779226\ttrain-auc:0.8215\n",
      "[1]\teval-auc:0.781528\ttrain-auc:0.825961\n",
      "[2]\teval-auc:0.783613\ttrain-auc:0.829467\n",
      "[3]\teval-auc:0.789654\ttrain-auc:0.831055\n",
      "[4]\teval-auc:0.795079\ttrain-auc:0.832715\n",
      "[5]\teval-auc:0.796618\ttrain-auc:0.833589\n",
      "[6]\teval-auc:0.797567\ttrain-auc:0.834818\n",
      "[7]\teval-auc:0.798396\ttrain-auc:0.835552\n",
      "[8]\teval-auc:0.79646\ttrain-auc:0.837819\n",
      "[9]\teval-auc:0.796137\ttrain-auc:0.839325\n",
      "Blind Log Loss: 0.49392594726767486\n",
      "Blind ROC: 0.7961370148630256\n",
      "finished a training model\n",
      "fitting on full data set now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoguai/anaconda3/envs/sklearn/lib/python3.6/site-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase memory consumption\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 4\n",
      "[0]\teval-auc:0.801286\ttrain-auc:0.821889\n",
      "[1]\teval-auc:0.812263\ttrain-auc:0.825834\n",
      "[2]\teval-auc:0.816714\ttrain-auc:0.830877\n",
      "[3]\teval-auc:0.817473\ttrain-auc:0.832079\n",
      "[4]\teval-auc:0.821068\ttrain-auc:0.834969\n",
      "[5]\teval-auc:0.823229\ttrain-auc:0.83667\n",
      "[6]\teval-auc:0.824621\ttrain-auc:0.836901\n",
      "[7]\teval-auc:0.825458\ttrain-auc:0.837343\n",
      "[8]\teval-auc:0.824894\ttrain-auc:0.838034\n",
      "[9]\teval-auc:0.82467\ttrain-auc:0.838986\n",
      "Blind Log Loss: 0.49359754970656416\n",
      "Blind ROC: 0.8246701744311183\n",
      "finished a training model\n",
      "fitting on full data set now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoguai/anaconda3/envs/sklearn/lib/python3.6/site-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase memory consumption\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 5\n",
      "[0]\teval-auc:0.817661\ttrain-auc:0.816289\n",
      "[1]\teval-auc:0.818437\ttrain-auc:0.824787\n",
      "[2]\teval-auc:0.824175\ttrain-auc:0.827308\n",
      "[3]\teval-auc:0.823242\ttrain-auc:0.828236\n",
      "[4]\teval-auc:0.82777\ttrain-auc:0.830826\n",
      "[5]\teval-auc:0.836619\ttrain-auc:0.833031\n",
      "[6]\teval-auc:0.835362\ttrain-auc:0.835501\n",
      "[7]\teval-auc:0.834731\ttrain-auc:0.836303\n",
      "[8]\teval-auc:0.836016\ttrain-auc:0.836942\n",
      "[9]\teval-auc:0.838184\ttrain-auc:0.838867\n",
      "Blind Log Loss: 0.4937996398623257\n",
      "Blind ROC: 0.8381844565960791\n",
      "finished a training model\n",
      "fitting on full data set now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoguai/anaconda3/envs/sklearn/lib/python3.6/site-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase memory consumption\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 6\n",
      "[0]\teval-auc:0.798024\ttrain-auc:0.817064\n",
      "[1]\teval-auc:0.81098\ttrain-auc:0.82189\n",
      "[2]\teval-auc:0.810497\ttrain-auc:0.824778\n",
      "[3]\teval-auc:0.816763\ttrain-auc:0.830633\n",
      "[4]\teval-auc:0.819677\ttrain-auc:0.835548\n",
      "[5]\teval-auc:0.822073\ttrain-auc:0.837357\n",
      "[6]\teval-auc:0.820862\ttrain-auc:0.836281\n",
      "[7]\teval-auc:0.820939\ttrain-auc:0.836889\n",
      "[8]\teval-auc:0.822384\ttrain-auc:0.838222\n",
      "[9]\teval-auc:0.821242\ttrain-auc:0.839935\n",
      "Blind Log Loss: 0.49381750898513627\n",
      "Blind ROC: 0.8212421181097024\n",
      "finished a training model\n",
      "fitting on full data set now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoguai/anaconda3/envs/sklearn/lib/python3.6/site-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase memory consumption\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 7\n",
      "[0]\teval-auc:0.809979\ttrain-auc:0.818891\n",
      "[1]\teval-auc:0.812038\ttrain-auc:0.820903\n",
      "[2]\teval-auc:0.820035\ttrain-auc:0.827355\n",
      "[3]\teval-auc:0.818185\ttrain-auc:0.826191\n",
      "[4]\teval-auc:0.829449\ttrain-auc:0.832155\n",
      "[5]\teval-auc:0.832453\ttrain-auc:0.833507\n",
      "[6]\teval-auc:0.830299\ttrain-auc:0.832733\n",
      "[7]\teval-auc:0.829449\ttrain-auc:0.833273\n",
      "[8]\teval-auc:0.830948\ttrain-auc:0.834106\n",
      "[9]\teval-auc:0.832619\ttrain-auc:0.835899\n",
      "Blind Log Loss: 0.4936103325781149\n",
      "Blind ROC: 0.8326188420918993\n",
      "finished a training model\n",
      "fitting on full data set now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoguai/anaconda3/envs/sklearn/lib/python3.6/site-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase memory consumption\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 8\n",
      "[0]\teval-auc:0.835477\ttrain-auc:0.818797\n",
      "[1]\teval-auc:0.841833\ttrain-auc:0.82628\n",
      "[2]\teval-auc:0.843104\ttrain-auc:0.829462\n",
      "[3]\teval-auc:0.843898\ttrain-auc:0.831376\n",
      "[4]\teval-auc:0.845291\ttrain-auc:0.832395\n",
      "[5]\teval-auc:0.849459\ttrain-auc:0.833487\n",
      "[6]\teval-auc:0.849748\ttrain-auc:0.83464\n",
      "[7]\teval-auc:0.848143\ttrain-auc:0.834495\n",
      "[8]\teval-auc:0.848919\ttrain-auc:0.835191\n",
      "[9]\teval-auc:0.848375\ttrain-auc:0.836302\n",
      "Blind Log Loss: 0.4933071628632404\n",
      "Blind ROC: 0.8483751144998568\n",
      "finished a training model\n",
      "fitting on full data set now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoguai/anaconda3/envs/sklearn/lib/python3.6/site-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase memory consumption\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 9\n",
      "[0]\teval-auc:0.815119\ttrain-auc:0.818708\n",
      "[1]\teval-auc:0.816439\ttrain-auc:0.821709\n",
      "[2]\teval-auc:0.819236\ttrain-auc:0.82868\n",
      "[3]\teval-auc:0.820221\ttrain-auc:0.828947\n",
      "[4]\teval-auc:0.820632\ttrain-auc:0.830498\n",
      "[5]\teval-auc:0.821824\ttrain-auc:0.832299\n",
      "[6]\teval-auc:0.824613\ttrain-auc:0.833981\n",
      "[7]\teval-auc:0.825032\ttrain-auc:0.834215\n",
      "[8]\teval-auc:0.825546\ttrain-auc:0.834775\n",
      "[9]\teval-auc:0.827695\ttrain-auc:0.836077\n",
      "Blind Log Loss: 0.4935291192865955\n",
      "Blind ROC: 0.8276950646030224\n",
      "finished a training model\n",
      "fitting on full data set now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoguai/anaconda3/envs/sklearn/lib/python3.6/site-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase memory consumption\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 10\n",
      "[0]\teval-auc:0.791223\ttrain-auc:0.824929\n",
      "[1]\teval-auc:0.797652\ttrain-auc:0.831144\n",
      "[2]\teval-auc:0.801832\ttrain-auc:0.832629\n",
      "[3]\teval-auc:0.803331\ttrain-auc:0.832796\n",
      "[4]\teval-auc:0.805368\ttrain-auc:0.835988\n",
      "[5]\teval-auc:0.806129\ttrain-auc:0.835941\n",
      "[6]\teval-auc:0.80654\ttrain-auc:0.835957\n",
      "[7]\teval-auc:0.808174\ttrain-auc:0.836883\n",
      "[8]\teval-auc:0.810074\ttrain-auc:0.837826\n",
      "[9]\teval-auc:0.812566\ttrain-auc:0.839174\n",
      "Blind Log Loss: 0.4941798559051958\n",
      "Blind ROC: 0.8125658585581884\n",
      "finished a training model\n",
      "fitting on full data set now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoguai/anaconda3/envs/sklearn/lib/python3.6/site-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase memory consumption\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_preds = None\n",
    "test_preds = None\n",
    "xgb_classifiers = [] # List[[clf, score]]\n",
    "index = 0\n",
    "for train_index, test_index in skf.split(train, target):\n",
    "    print('Fold:', index + 1)\n",
    "    index = index + 1\n",
    "    X_train, X_test = train.iloc[train_index], train.iloc[test_index] # split training set into'train', 'cross-validation' sets\n",
    "#    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
    "\n",
    "    # for xgb classifier, we transform them into DMatrix format\n",
    "    D_train = xgb.DMatrix(\n",
    "                    csr_matrix(X_train[features]),\n",
    "                    X_train.target.values,\n",
    "                    silent = True)\n",
    "#    print(D_train.num_col())\n",
    "#    print(D_train.num_row())\n",
    "    \n",
    "    D_test = xgb.DMatrix(\n",
    "                    csr_matrix(X_test[features]),\n",
    "                    X_test.target.values,\n",
    "                    silent = True)\n",
    "    watchlist = [(D_test, 'eval'), (D_train, 'train')]\n",
    "#    print(D_test.num_col())\n",
    "#    print(D_test.num_row())\n",
    "  \n",
    "    # fit the classfier now\n",
    "    clf = xgb.train(params, D_train, num_boost_round,\n",
    "                    evals = watchlist)\n",
    "    \n",
    "    test_prediction = clf.predict(D_test)\n",
    "    print('Blind Log Loss:', log_loss(X_test.target.values, test_prediction))\n",
    "    score = roc_auc_score(X_test.target.values, test_prediction)\n",
    "    print('Blind ROC:', score)\n",
    "    \n",
    "    \n",
    "    del X_train, X_test, D_train, D_test\n",
    "    gc.collect()\n",
    "    print('finished a training model')\n",
    "    print('fitting on full data set now...')\n",
    "    \n",
    "    D_full_train = \\\n",
    "        xgb.DMatrix(csr_matrix(train[features]),\n",
    "                    train.values,\n",
    "                    silent = True)\n",
    "    D_full_test = \\\n",
    "        xgb.DMatrix(csr_matrix(test[features]),\n",
    "                    silent = True)\n",
    "    if(train_preds is None):\n",
    "        train_preds = clf.predict(D_full_train)\n",
    "        test_preds = clf.predict(D_full_test)\n",
    "    else:\n",
    "        train_preds *= clf.predict(D_full_train) # we manually perform an average of the results\n",
    "        test_preds *= clf.predict(D_full_test)\n",
    "    xgb_classifiers.append([clf, 'with auc score: {:10f}'.format(score)])\n",
    "    del D_full_train, D_full_test, clf\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the xgb_classifiers lists for later ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgb_classifiers, open('xgboost_classifier_param1.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('xgboost_classifier_param1.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set another set of parameters, use a randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 5\n",
    "random_state = 111\n",
    "skf = StratifiedKFold(target,\n",
    "                      n_folds=split,\n",
    "                      shuffle=False,\n",
    "                      random_state= random_state)\n",
    "num_rounds = 350 + np.random.randint(low = -50, high = 50)\n",
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eta\"] = 0.03 + np.random.normal(loc = 0.0, scale = 0.01)\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.7\n",
    "params[\"silent\"] = 1\n",
    "params[\"max_depth\"] = 5 + np.random.randint(low = -1, high = 2)\n",
    "params[\"min_child_weight\"] = 1\n",
    "params[\"eval_metric\"] = \"auc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fold:', 0)\n",
      "('Blind Log Loss:', 0.13515509804987907)\n",
      "('Blind ROC:', 0.83580701685336112)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 1)\n",
      "('Blind Log Loss:', 0.13680937251164763)\n",
      "('Blind ROC:', 0.82864225095512389)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 2)\n",
      "('Blind Log Loss:', 0.13302371896058762)\n",
      "('Blind ROC:', 0.84442239514816397)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 3)\n",
      "('Blind Log Loss:', 0.12968091185474287)\n",
      "('Blind ROC:', 0.85590906677247269)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 4)\n",
      "('Blind Log Loss:', 0.13412376736040632)\n",
      "('Blind ROC:', 0.83843641868857111)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_preds = None\n",
    "test_preds = None\n",
    "xgb_classifiers2 = []\n",
    "for index, (train_index, test_index) in enumerate(skf):\n",
    "    print('Fold:', index)\n",
    "    X_train = train.iloc[train_index] # split training set into'train', 'cross-validation' sets\n",
    "    X_test = train.iloc[test_index]\n",
    "    \n",
    "    # for xgb classifier, we transform them into DMatrix format\n",
    "    D_train = xgb.DMatrix(\n",
    "                    csr_matrix(X_train[features]),\n",
    "                    X_train.target.values,\n",
    "                    silent=True)\n",
    "    \n",
    "    D_test = xgb.DMatrix(\n",
    "                    csr_matrix(X_test[features]),\n",
    "                    X_test.target.values,\n",
    "                    silent=True)\n",
    "    watchlist = [(D_test, 'eval'), (D_train, 'train')]\n",
    "    \n",
    "    # fit the classfier now\n",
    "    clf = xgb.train(params, D_train, num_rounds,\n",
    "                    evals = watchlist, early_stopping_rounds=50,\n",
    "                    verbose_eval=False)\n",
    "\n",
    "    test_prediction = clf.predict(D_test)\n",
    "    print('Blind Log Loss:', log_loss(X_test.target.values,\n",
    "                                      test_prediction))\n",
    "    score = roc_auc_score(X_test.target.values,\n",
    "                                      test_prediction)\n",
    "    print('Blind ROC:', score)\n",
    "    index = index + 1\n",
    "    \n",
    "    del X_train, X_test, D_train, D_test\n",
    "    gc.collect()\n",
    "    print 'finished a training model'\n",
    "    print 'fitting on full data set now...'\n",
    "    \n",
    "    D_full_train = \\\n",
    "        xgb.DMatrix(csr_matrix(train[features]),\n",
    "                    train.target.values,\n",
    "                    silent=True)\n",
    "    D_full_test = \\\n",
    "        xgb.DMatrix(csr_matrix(test[features]),\n",
    "                    silent=True)\n",
    "    if(train_preds is None):\n",
    "        train_preds = clf.predict(D_full_train)\n",
    "        test_preds = clf.predict(D_full_test)\n",
    "    else:\n",
    "        train_preds *= clf.predict(D_full_train) # we manually perform an average of the results\n",
    "        test_preds *= clf.predict(D_full_test)\n",
    "    xgb_classifiers2.append([clf, 'with auc score: {:10f}'.format(score)])\n",
    "    del D_full_train, D_full_test, clf\n",
    "    gc.collect()\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.7, 'silent': 1, 'eval_metric': 'auc', 'min_child_weight': 1, 'subsample': 0.8, 'eta': 0.019899065033745854, 'objective': 'binary:logistic', 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "print params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Log Loss: 0.49344423582242747\n",
      "Average ROC: 0.8404411930500204\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.power(train_preds, 1./split)\n",
    "test_preds = np.power(test_preds, 1./split)\n",
    "print('Average Log Loss:', log_loss(train.target.values, train_preds))\n",
    "print('Average ROC:', roc_auc_score(train.target.values, train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({\"ID\": train.ID,\n",
    "                           \"TARGET\": train.target,\n",
    "                           \"PREDICTION\": train_preds})\n",
    "\n",
    "submission.to_csv(\"simplexgbtrain.csv\", index=False)\n",
    "submission = pd.DataFrame({\"ID\": test.ID, \"TARGET\": test_preds})\n",
    "submission.to_csv(\"simplexgbtest.csv\", index=False)\n",
    "print('Finish')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
